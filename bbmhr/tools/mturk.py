import json
import numpy as np
import os
import csv

from typing import Dict, List, Text

def read_self_chat(path: Text):
    """Read from jsonl self chat file generated by ParlAI

    Args:
        path (Text): directory contains all the jsonl files.
    """
    jsonl_files = []
    for (dirpath, dirnames, filenames) in os.walk(path):
        for filename in filenames:
            if filename.endswith(".jsonl"):
                jsonl_files.append(filename)
    
    print(jsonl_files)

    for file in jsonl_files:
        origin_file_name = file.replace(".jsonl", ".txt")
        origin_file = open(origin_file_name, 'w+', encoding='utf-8')
        self_chat_file = open(os.path.join(path, file), 'r', encoding='utf-8')
        for line in self_chat_file.readlines():
            chat_data = json.loads(line)
            for utterance in chat_data['dialog']:
                origin_file.write('seeker: ' + utterance[0]['text'].split(" The seeker ")[0] + "\n")
                origin_file.write('supporter: ' + utterance[1]['text'] + "\n")
 

def generate_batch(task_name: Text, write_to=False):
    """Generate conversation pairs for ACUTE-Eval conversation_1 is human
    conversation_2 is model generated.

    Args:
        task_name (Text): Task name/model name
        write_to (bool, optional): Save to files or not

    Returns:
        _type_: Return the conversation pairs as a dict
    """
    dest_file_name = "./eval/self_chat/batch_origin/" + task_name + ".csv"
    if write_to:
        dest_file = open(dest_file_name, 'w+', encoding='utf-8', newline='')
        fieldnames = ['conversation_1', 'conversation_2']
        writer = csv.DictWriter(dest_file, fieldnames=fieldnames)

    for (dirpath, dirnames, filenames) in os.walk("./eval/self_chat/batch_origin/"):
        print(filenames)
    text_dict = {}
    for filename in filenames:
        if ".xlsx" in filename or ".csv" in filename:
            continue
        text = open("./eval/self_chat/batch_origin/" + filename, 'r', encoding='utf-8').read()

        if "human_" not in filename:
            text = text.replace("seeker","<strong>seeker</strong>")
            text = text.replace("supporter",'<strong style="background-color: aqua">supporter 2</strong>')
            text = text.replace("\n","<br>\n")
        text_dict[filename] = text
    
    if write_to:
        writer.writeheader()
        writer.writerow({"conversation_1": text_dict["human_academic.txt"], "conversation_2": text_dict[task_name + "_academic.txt"]})
        writer.writerow({"conversation_1": text_dict["human_friend.txt"], "conversation_2": text_dict[task_name + "_friend.txt"]})
        writer.writerow({"conversation_1": text_dict["human_job.txt"], "conversation_2": text_dict[task_name + "_job.txt"]})
        writer.writerow({"conversation_1": text_dict["human_ongoing.txt"], "conversation_2": text_dict[task_name + "_ongoing.txt"]})
        writer.writerow({"conversation_1": text_dict["human_partner.txt"], "conversation_2": text_dict[task_name + "_partner.txt"]})

    return text_dict


def fleiss_kappa(ratings, n):
    '''
    Computes the Fleiss' kappa measure for assessing the reliability of 
    agreement between a fixed number n of raters when assigning categorical
    ratings to a number of items.
    
    Args:
        ratings: a list of (item, category)-ratings
        n: number of raters
        k: number of categories
    Returns:
        the Fleiss' kappa score
    
    See also:
        http://en.wikipedia.org/wiki/Fleiss'_kappa
    '''
    items = set()
    categories = set()
    n_ij = {}
    
    for i, c in ratings:
        items.add(i)
        categories.add(c)
        n_ij[(i,c)] = n_ij.get((i,c), 0) + 1
    
    N = len(items)
    
    p_j = dict(((c, sum(n_ij.get((i, c), 0) for i in items) / (1.0 * n * N)) for c in categories))
    P_i = dict(((i, (sum(n_ij.get((i, c), 0) ** 2 for c in categories) - n) / (n * (n - 1.0))) for i in items))

    P_bar = sum(P_i.values()) / (1.0 * N)
    P_e_bar = sum(value ** 2 for value in p_j.values())
    
    kappa = (P_bar - P_e_bar) / (1 - P_e_bar)
    
    return kappa


def read_results(results_task: Text, text_dict: Dict[Text,Text]):
    """Read from turk result csv

    Args:
        results_task (Text): task name.
        text_dict (Dict[Text,Text]): The original conversation pairs. Used to
        identify the instance.

    Returns:
        _type_: Returns a results of all five topics as a dict and raw results as a list.
    """
    results_file = './eval/Turkresults/self_chat/human_' + results_task + '.csv'
    csv_file = open(results_file, newline='', encoding='utf-8')
    reader = csv.DictReader(csv_file, delimiter=',')
    results = {
        'academic': [],
        'friend': [],
        'job': [],
        'ongoing': [],
        'partner': []
    }
    bad_annotators = []
    bad_path = './eval/Turkresults/workers/' + results_task + '/bad workers' 
    print(f"Loading bad annotators from: {bad_path}")
    with open(bad_path, 'r', newline='') as file:
        bad_annotators = [line.strip() for line in file.readlines()]
    print(bad_annotators)
    raw_results = []
    for row in reader:
        if row["WorkerId"] in bad_annotators or row["RejectionTime"] != "":
            continue
        annotator = [row["WorkerId"]]
        print(row["RejectionTime"])
        for question_number in range(1,18):
            if row[f"Answer.q{question_number}_1.s1"] == 'true':
                annotator.append(0)
            elif row[f"Answer.q{question_number}_2.s2"] == 'true':
                annotator.append(1)
            
        for tag_number in range(1,7):
            annotator.append(row[f"Answer.tag{tag_number}"])
        
        annotator.append(row["WorkTimeInSeconds"])

        raw_results.append(annotator.copy())
        if row['Input.conversation_2'] == text_dict[results_task + '_academic.txt']:
            results["academic"].append(annotator.copy()[1:18])
        elif row['Input.conversation_2'] == text_dict[results_task + '_friend.txt']:
            results["friend"].append(annotator.copy()[1:18])
        elif row['Input.conversation_2'] == text_dict[results_task + '_job.txt']:
            results["job"].append(annotator.copy()[1:18])
        elif row['Input.conversation_2'] == text_dict[results_task + '_ongoing.txt']:
            results["ongoing"].append(annotator.copy()[1:18])
        elif row['Input.conversation_2'] == text_dict[results_task + '_partner.txt']:
            results["partner"].append(annotator.copy()[1:18])
    
    return results, raw_results


def bad_annotators(raw_results: List[List]):
    """List all annotators with their choices and justifications, working time.

    Args:
        raw_results (List[List]): The raw turking results as a list.
    """
    workers = {}
    for item in raw_results:
        if item[0] in workers:
            workers[item[0]].append(item[1:])
        else:
            workers[item[0]] = [item[1:]]

    for worker, values in workers.items():
        print(worker)
        for value in values:
            print(value)
        print('\n')


def latex_line(data, style):
    """Generate a latex table line with specific style

    Args:
        data (_type_): A list of input data.
        style (_type_): Style
    """
    result = ""
    if style == "xx.x":
        for item in data:
            print(item * 100)
            if item > 0:
                result += "         &  + " + str(round(item * 100, 2))
            else:
                result += "         &  - " + str(round(-item * 100, 2))
    print(result)


def prove_reject(task: Text):
    """Prove or reject the turk HITs. If the WorkerId is in bad_annotators, reject, approve
    otherwise.

    Args:
        task (Text): Task name.
    """
    results_file = './eval/Turkresults/self_chat/human_' + task + '.csv'
    csv_file = open(results_file, 'rt', newline='', encoding='utf-8')
    reader = csv.reader(csv_file, delimiter=',')
    fieldnames = next(reader)
    csv_file.close()
    print(fieldnames)
    csv_file = open(results_file, 'rt', newline='', encoding='utf-8')
    dict_reader = csv.DictReader(csv_file, delimiter=',')

    decision_path = './eval/Turkresults/self_chat/decision/human_' + task + '.csv'
    decision_file = open(decision_path, 'w+', newline='', encoding='utf-8')

    bad_annotators = []
    common_bad_path = './eval/Turkresults/bad workers'
    task_bad_path = './eval/Turkresults/workers/' + task + '/bad workers' 
    print(f"Loading bad annotators from: {common_bad_path}")
    with open(common_bad_path, 'r', newline='') as file:
        bad_annotators = [line.strip() for line in file.readlines()]
    print(bad_annotators)
    print(f"Loading bad annotators from: {task_bad_path}")
    with open(task_bad_path, 'r', newline='') as file:
        bad_annotators.extend([line.strip() for line in file.readlines()])

    print(bad_annotators)

    
    writer = csv.DictWriter(decision_file, fieldnames=fieldnames)
    writer.writeheader()

    for line in dict_reader:
        # print(line)
        if line["WorkerId"] in bad_annotators or line["RejectionTime"] != "":
            line["Reject"] = "You answers are not valid as they are one-sided. Your justifications are invalid."
        else:
            line["Approve"] = "x"
        writer.writerow(line)


def main():
    task_name = 'bbmh'
    # read_self_chat("./eval/self_chat")
    text_dict = generate_batch(task_name)
    # generate_batch("gpt_1")
    # generate_batch("gpt_2")
    # generate_batch("ada", write_to=True)
    # generate_batch("davinci", write_to=True)
    # generate_batch("bbmh")
    dict_results, raw_results = read_results(task_name, text_dict)
    for key, value in dict_results.items():
        print(len(value))
    bad_annotators(raw_results)

    wins = []
    question_wins = []
    section_wins = []
    for key, value in dict_results.items():
        full_score = np.full((1, 17), len(value))
        sys_2 = np.sum(value, axis=0)
        sys_1 = np.subtract(full_score, sys_2)[0]

        sys_1_win = np.subtract(sys_1, sys_2)
        question_win_rate = sys_1_win / len(value)
        question_wins.append(question_win_rate.tolist())
        section_wins.append(
            [
                np.sum(question_win_rate[0:2]) / 3,
                np.sum(question_win_rate[3:5]) / 3,
                np.sum(question_win_rate[6:8]) / 3,
                np.sum(question_win_rate[9:11]) / 3,
                np.sum(question_win_rate[12:14]) / 3,
                np.sum(question_win_rate[15:16]) / 2
            ]
        )
        wins.append(np.sum(sys_1_win) / (len(value) * 17))
    print(np.sum(section_wins, axis=0) / 5)

    latex_line((np.sum(section_wins, axis=0) / 5).tolist(), style='xx.x')

    print( round(sum(wins) / len(wins) * 100, 2) )

if __name__ == "__main__":
    # main()
    # task_name = "davinci"
    # prove_reject(task_name)
    main()
